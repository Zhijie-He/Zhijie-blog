<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>教程 | Zhijie He&#39;s Blog</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="Zhijie He&#39;s learning blogs in computer science">
    
    <link rel="preload" href="/learning-blogs/assets/css/0.styles.f8b47888.css" as="style"><link rel="preload" href="/learning-blogs/assets/js/app.023149a4.js" as="script"><link rel="preload" href="/learning-blogs/assets/js/2.4cc10ed0.js" as="script"><link rel="preload" href="/learning-blogs/assets/js/1.91ea3048.js" as="script"><link rel="preload" href="/learning-blogs/assets/js/48.fdc47c13.js" as="script"><link rel="prefetch" href="/learning-blogs/assets/js/10.4d0079fb.js"><link rel="prefetch" href="/learning-blogs/assets/js/11.a25d570d.js"><link rel="prefetch" href="/learning-blogs/assets/js/12.d87c0ace.js"><link rel="prefetch" href="/learning-blogs/assets/js/13.1a43b501.js"><link rel="prefetch" href="/learning-blogs/assets/js/14.6a978a6e.js"><link rel="prefetch" href="/learning-blogs/assets/js/15.81b6c8b5.js"><link rel="prefetch" href="/learning-blogs/assets/js/16.3637d4f7.js"><link rel="prefetch" href="/learning-blogs/assets/js/17.62ca55f1.js"><link rel="prefetch" href="/learning-blogs/assets/js/18.c36d9be8.js"><link rel="prefetch" href="/learning-blogs/assets/js/19.d6620149.js"><link rel="prefetch" href="/learning-blogs/assets/js/20.a728a170.js"><link rel="prefetch" href="/learning-blogs/assets/js/21.2f0a0271.js"><link rel="prefetch" href="/learning-blogs/assets/js/22.6180816b.js"><link rel="prefetch" href="/learning-blogs/assets/js/23.878596c4.js"><link rel="prefetch" href="/learning-blogs/assets/js/24.8056bf81.js"><link rel="prefetch" href="/learning-blogs/assets/js/25.0b213a97.js"><link rel="prefetch" href="/learning-blogs/assets/js/26.5e1034dc.js"><link rel="prefetch" href="/learning-blogs/assets/js/27.91e51ec8.js"><link rel="prefetch" href="/learning-blogs/assets/js/28.065d51b5.js"><link rel="prefetch" href="/learning-blogs/assets/js/29.6e23176b.js"><link rel="prefetch" href="/learning-blogs/assets/js/3.bd89fe37.js"><link rel="prefetch" href="/learning-blogs/assets/js/30.fc97d9e8.js"><link rel="prefetch" href="/learning-blogs/assets/js/31.a4fef5bd.js"><link rel="prefetch" href="/learning-blogs/assets/js/32.304378ad.js"><link rel="prefetch" href="/learning-blogs/assets/js/33.5b418366.js"><link rel="prefetch" href="/learning-blogs/assets/js/34.8c1d4ab1.js"><link rel="prefetch" href="/learning-blogs/assets/js/35.7309b6e5.js"><link rel="prefetch" href="/learning-blogs/assets/js/36.e9ce87c9.js"><link rel="prefetch" href="/learning-blogs/assets/js/37.f17d44cc.js"><link rel="prefetch" href="/learning-blogs/assets/js/38.82558ec4.js"><link rel="prefetch" href="/learning-blogs/assets/js/39.bfee11e0.js"><link rel="prefetch" href="/learning-blogs/assets/js/4.7b60af58.js"><link rel="prefetch" href="/learning-blogs/assets/js/40.c9c50cde.js"><link rel="prefetch" href="/learning-blogs/assets/js/41.b2a2cca9.js"><link rel="prefetch" href="/learning-blogs/assets/js/42.102500cb.js"><link rel="prefetch" href="/learning-blogs/assets/js/43.be7ac4cf.js"><link rel="prefetch" href="/learning-blogs/assets/js/44.9fe9b32a.js"><link rel="prefetch" href="/learning-blogs/assets/js/45.bea8e4aa.js"><link rel="prefetch" href="/learning-blogs/assets/js/46.ec621935.js"><link rel="prefetch" href="/learning-blogs/assets/js/47.b7fae7bb.js"><link rel="prefetch" href="/learning-blogs/assets/js/49.c8489b22.js"><link rel="prefetch" href="/learning-blogs/assets/js/5.8c2caae0.js"><link rel="prefetch" href="/learning-blogs/assets/js/50.ee71df54.js"><link rel="prefetch" href="/learning-blogs/assets/js/51.c82918e2.js"><link rel="prefetch" href="/learning-blogs/assets/js/52.5370e504.js"><link rel="prefetch" href="/learning-blogs/assets/js/53.31e86553.js"><link rel="prefetch" href="/learning-blogs/assets/js/54.063a693e.js"><link rel="prefetch" href="/learning-blogs/assets/js/55.12da9be3.js"><link rel="prefetch" href="/learning-blogs/assets/js/56.ab455507.js"><link rel="prefetch" href="/learning-blogs/assets/js/6.af045fac.js"><link rel="prefetch" href="/learning-blogs/assets/js/7.f22cdc7f.js"><link rel="prefetch" href="/learning-blogs/assets/js/vendors~docsearch.11055bee.js">
    <link rel="stylesheet" href="/learning-blogs/assets/css/0.styles.f8b47888.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/learning-blogs/" class="home-link router-link-active"><!----> <span class="site-name">Zhijie He's Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/Zhijie-He/learning-blogs" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/Zhijie-He/learning-blogs" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><a href="/learning-blogs/" class="sidebar-heading clickable router-link-active"><span>欢迎学习</span> <!----></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/learning-blogs/" aria-current="page" class="sidebar-link">学前必读</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/vuepress" class="sidebar-heading clickable"><span>VuePress</span> <span class="arrow right"></span></a> <!----></section></li><li><a href="/learning-blogs/markdown/" class="sidebar-link">Markdown 教程</a></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/miniprograms" class="sidebar-heading clickable"><span>微信小程序</span> <span class="arrow right"></span></a> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/phd" class="sidebar-heading clickable"><span>博士</span> <span class="arrow right"></span></a> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/coding-tips" class="sidebar-heading clickable"><span>编程技巧</span> <span class="arrow right"></span></a> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/pytorch" class="sidebar-heading clickable router-link-active open"><span>PyTorch</span> <span class="arrow down"></span></a> <ul class="sidebar-links sidebar-group-items"><li><a href="/learning-blogs/pytorch/" aria-current="page" class="sidebar-link">安装</a></li><li><a href="/learning-blogs/pytorch/tutorial.html" aria-current="page" class="active sidebar-link">教程</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#tensors-张量" class="sidebar-link">Tensors 张量</a></li><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#datasets-dataloaders" class="sidebar-link">Datasets &amp; DataLoaders</a></li><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#transforms" class="sidebar-link">Transforms</a></li><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#构建神经网络模型" class="sidebar-link">构建神经网络模型</a></li><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#autograd" class="sidebar-link">Autograd</a></li><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#optimization" class="sidebar-link">Optimization</a></li><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#保存和加载模型" class="sidebar-link">保存和加载模型</a></li><li class="sidebar-sub-header"><a href="/learning-blogs/pytorch/tutorial.html#代码复现" class="sidebar-link">代码复现</a></li></ul></li><li><a href="/learning-blogs/pytorch/hyperparameter_tuning.html" class="sidebar-link">超参数优化</a></li><li><a href="/learning-blogs/pytorch/optimizer.html" class="sidebar-link">optimizer</a></li><li><a href="/learning-blogs/pytorch/transforms.html" class="sidebar-link">transforms v1 and v2</a></li><li><a href="/learning-blogs/pytorch/applications.html" class="sidebar-link">应用</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/pytorch-timm" class="sidebar-heading clickable"><span>PyTorch Timm</span> <span class="arrow right"></span></a> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/pytorch-lightning" class="sidebar-heading clickable"><span>PyTorch Lightning</span> <span class="arrow right"></span></a> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><a href="/learning-blogs/UvA-DL-notebooks" class="sidebar-heading clickable"><span>深度学习教程</span> <span class="arrow right"></span></a> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="教程"><a href="#教程" class="header-anchor">#</a> 教程</h1> <h2 id="tensors-张量"><a href="#tensors-张量" class="header-anchor">#</a> Tensors 张量</h2> <blockquote><p>Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.</p></blockquote> <p>tensors和numpy的多维数组很相似，区别是tensors可以运行在GPUs以及其他的硬件加速器上。</p> <blockquote><p>In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data.
Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd section).</p></blockquote> <h3 id="代码可复现"><a href="#代码可复现" class="header-anchor">#</a> 代码可复现</h3> <div class="language-python extra-class"><pre class="language-python"><code>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span> <span class="token comment"># Setting the seed</span>
</code></pre></div><h3 id="创建tensor"><a href="#创建tensor" class="header-anchor">#</a> 创建tensor</h3> <ol><li>Directly from data, data type会自动推断</li></ol> <div class="language-python extra-class"><pre class="language-python"><code>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre></div><ol start="2"><li>from Numpy Array</li></ol> <div class="language-python extra-class"><pre class="language-python"><code>np_array <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
x_np <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np_array<span class="token punctuation">)</span>
</code></pre></div><ol start="3"><li>from another tensor
新的张量会保存参数张量的特性,shape和datatype, 除非特别指定。</li></ol> <div class="language-python extra-class"><pre class="language-python"><code>x_ones <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x_data<span class="token punctuation">)</span> <span class="token comment"># retains the properties of x_data</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Ones Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>x_ones<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>

x_rand <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token comment"># overrides the datatype of x_data</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Random Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>x_rand<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>Ones Tensor<span class="token punctuation">:</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

Random Tensor<span class="token punctuation">:</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.4223</span><span class="token punctuation">,</span> <span class="token number">0.1719</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.3184</span><span class="token punctuation">,</span> <span class="token number">0.2631</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><ol start="4"><li>给定张量的维度,生成随机或者固定值
常用方法：</li></ol> <ul><li>torch.zeros: 创建一个值全为0的张量</li> <li>torch.ones: 创建一个值全为1的张量</li> <li>torch.rand: 创建一个张量，其随机值在 0 和 1 之间均匀采样</li> <li>torch.randn: 创建一个张量，其中包含从平均值为 0、方差为 1 的正态分布中采样的随机值</li> <li>torch.arange: Creates a tensor containing the values</li> <li>torch.Tensor (input list): Creates a tensor from the list elements you provide</li></ul> <div class="language-python extra-class"><pre class="language-python"><code>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><p>shape是一个tuple代表张量的维度</p> <div class="language-python extra-class"><pre class="language-python"><code>shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
rand_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
ones_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
zeros_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Random Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>rand_tensor<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Ones Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>ones_tensor<span class="token punctuation">}</span></span><span class="token string"> \n&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Zeros Tensor: \n </span><span class="token interpolation"><span class="token punctuation">{</span>zeros_tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre></div><h3 id="tensor-to-numpy-numpy-to-tensor"><a href="#tensor-to-numpy-numpy-to-tensor" class="header-anchor">#</a> Tensor to Numpy, Numpy to Tensor</h3> <div class="language-python extra-class"><pre class="language-python"><code>np_arr <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np_arr<span class="token punctuation">)</span>

tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
np_arr <span class="token operator">=</span> tensor<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><p>注意：当使用tensor.numpy()方法的时候需要确保tensor在CPU上，如果在GPU上，需要先调用.cpu()方法，即 <code>np_arr = tensor.cpu().numpy()</code>.</p> <h3 id="张量的属性"><a href="#张量的属性" class="header-anchor">#</a> 张量的属性</h3> <p>Tensor attributes describe their shape, datatype, and the device on which they are stored.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Shape of tensor: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
Shape of tensor<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Datatype of tensor: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">.</span>dtype<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
Datatype of tensor<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>float32
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Device tensor is stored on: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">.</span>device<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
Device tensor <span class="token keyword">is</span> stored on<span class="token punctuation">:</span> cpu
</code></pre></div><p>对于tensor的shape除了可以使用和numpy中.shape一样的方式，还可与你使用<code>.size()</code>方法</p> <div class="language-python extra-class"><pre class="language-python"><code>size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Size:&quot;</span><span class="token punctuation">,</span> size<span class="token punctuation">)</span>

dim1<span class="token punctuation">,</span> dim2<span class="token punctuation">,</span> dim3 <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Size:&quot;</span><span class="token punctuation">,</span> dim1<span class="token punctuation">,</span> dim2<span class="token punctuation">,</span> dim3<span class="token punctuation">)</span>

Size<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Size<span class="token punctuation">:</span> <span class="token number">2</span> <span class="token number">3</span> <span class="token number">4</span>
</code></pre></div><h3 id="张量操作"><a href="#张量操作" class="header-anchor">#</a> 张量操作</h3> <p>Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer">here<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.
所有的运算都可以在GPU上运行，这个速度比在CPU上快很多。默认情况下，张量在CPU上创建，如果需要移动到GPU上，需要使用<code>.to</code>方法。</p> <p><code>⚠️ 在跨deivces上复制较大的张量会严重降低速度和占用内存</code></p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># We move our tensor to the GPU if available</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>
</code></pre></div><ul><li>修改tensor的shape
size为 (2,3) 的张量可以重新组织为具有相同数量元素的任何其他形状（例如大小为 (6) 或 (3,2)...的张量）。在 PyTorch 中，这个操作为<code>view</code>：</li></ul> <div class="language-python extra-class"><pre class="language-python"><code>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;X&quot;</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>
X tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># Swapping dimension 0 and 1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;X&quot;</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>
X tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><ul><li>拥有和numpy array一样的下标索引与切片</li></ul> <div class="language-python extra-class"><pre class="language-python"><code>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'First row: '</span><span class="token punctuation">,</span>tensor<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># or tensor[0,:] tensor[0,...]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'First column: '</span><span class="token punctuation">,</span> tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># or tensor[..., 0]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Last column:'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># or tensor[:, -1]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'First two rows, last column'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Middle two rows'</span><span class="token punctuation">,</span> tensor<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 

tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
First row<span class="token punctuation">:</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
First column<span class="token punctuation">:</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Last column<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
First two rows<span class="token punctuation">,</span> last column tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Middle two rows tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><ul><li>合并张量
可以使用<code>torch.cat</code>将一串张量按照给定的维度进行合并。或者用<code>torch.stack</code>将张量合并，但是会产生新的维度。</li></ul> <blockquote><p>torch.cat (张量拼接)<br>
功能：torch.cat用于将一系列张量沿着现有的维度进行拼接。<br>
参数：主要参数包括一系列的张量和拼接的维度（dim）。<br>
用法示例：如果你有两个形状为（2, 3）的张量，使用torch.cat沿着第一个维度（dim=0）拼接，结果张量的形状将是（4, 3）。如果沿着第二个维度（dim=1）拼接，结果张量的形状将是（2, 6）。</p></blockquote> <blockquote><p>torch.stack (张量堆叠) <br>
功能：torch.stack用于将一系列张量堆叠成一个新的张量，这会创建一个新的维度。<br>
参数：主要参数包括一系列的张量和堆叠的维度（dim）。<br>
用法示例：如果你有两个形状为（2, 3）的张量，使用torch.stack沿着新的维度（假设为dim=0）堆叠，结果张量的形状将是（2, 2, 3）。这里，每个原始张量完整地保留，并作为新维度的一个切片。</p></blockquote> <p>注意：使用<code>torch.stack</code>时，所有的张量都必须具有一致的size. 这是因为 <code>torch.stack</code> 通过在新的维度上堆叠这些张量来创建一个更高维度的张量。为了保证这个新创建的张量的每个维度都是均匀和规整的，每个被堆叠的张量都必须有相同的形状.
对于 <code>torch.cat</code>（张量拼接）的使用，情况与 <code>torch.stack</code> 略有不同。在使用 <code>torch.cat</code> 时，输入张量<strong>在被拼接的维度（dim 参数指定的维度）可以有不同的大小，但在其他所有维度上，它们必须具有相同的尺寸</strong>。这样做是为了确保在非拼接维度上数据的一致性和整齐排列。</p> <p>假设我们有两个二维张量，一个形状为 (3, 4)，另一个形状为 (3, 2)。 这两个张量可以沿着第二维（列）进行拼接，因为它们在第一维（行）上具有相同的大小。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 定义两个张量，第一维大小相同</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># 沿第二维拼接这两个张量</span>
z <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># 输出：torch.Size([3, 6])</span>
</code></pre></div><p>在这个例子中，由于 x 和 y 在第一维（行）上的大小相同，它们可以沿着第二维进行拼接，结果是一个新的张量，其形状是 (3, 6)，这个新形状的第二维大小是原来两个张量第二维大小的和。</p> <h3 id="算数运算"><a href="#算数运算" class="header-anchor">#</a> 算数运算</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value</span>
y1 <span class="token operator">=</span> tensor @ tensor<span class="token punctuation">.</span>T
y2 <span class="token operator">=</span> tensor<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>T<span class="token punctuation">)</span>

y3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">.</span>T<span class="token punctuation">,</span> out<span class="token operator">=</span>y3<span class="token punctuation">)</span>


<span class="token comment"># This computes the element-wise product. z1, z2, z3 will have the same value</span>
z1 <span class="token operator">=</span> tensor <span class="token operator">*</span> tensor
z2 <span class="token operator">=</span> tensor<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

z3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">,</span> out<span class="token operator">=</span>z3<span class="token punctuation">)</span>
</code></pre></div><p>需要注意的是第三赋值方式，先根据目标张量创建具有相同维度的张量，然后将最后的结果赋值给他，通过<code>out=</code>这个参数。</p> <ul><li>单一值张量</li></ul> <blockquote><p>If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():</p></blockquote> <div class="language-python extra-class"><pre class="language-python"><code>agg <span class="token operator">=</span> tensor<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
agg_item <span class="token operator">=</span> agg<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token number">12.0</span> <span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'float'</span><span class="token operator">&gt;</span>
</code></pre></div><ul><li>In-place operations</li></ul> <blockquote><p>Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x.</p></blockquote> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> <span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><p>注意： In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.</p> <h3 id="tensors和numpy数组共享内存"><a href="#tensors和numpy数组共享内存" class="header-anchor">#</a> tensors和numpy数组共享内存</h3> <p>从Tensors到numpy 可以使用numpy()</p> <div class="language-python extra-class"><pre class="language-python"><code>t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;t: </span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
n <span class="token operator">=</span> t<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;n: </span><span class="token interpolation"><span class="token punctuation">{</span>n<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre></div><p>t: tensor([1., 1., 1., 1., 1.]) <br>
n: [1. 1. 1. 1. 1.]</p> <p>当改变tensors的值，numpy的值也对应改变。</p> <div class="language-python extra-class"><pre class="language-python"><code>t<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;t: </span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;n: </span><span class="token interpolation"><span class="token punctuation">{</span>n<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre></div><p>t: tensor([2., 2., 2., 2., 2.]) <br>
n: [2. 2. 2. 2. 2.]</p> <p>反过来也是一样，从numpy到tensors可以使用<code>torch.from_numpy()</code></p> <div class="language-python extra-class"><pre class="language-python"><code>n <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
t <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>n<span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code>np.add(n, 1, out=n)
print(f&quot;t: {t}&quot;)
print(f&quot;n: {n}&quot;)
</code></pre></div><p>t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)<br>
n: [2. 2. 2. 2. 2.]</p> <h2 id="datasets-dataloaders"><a href="#datasets-dataloaders" class="header-anchor">#</a> Datasets &amp; DataLoaders</h2> <ul><li>Image datasets <a href="https://pytorch.org/vision/stable/datasets.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/vision/stable/datasets.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Text datasets <a href="https://pytorch.org/text/stable/datasets.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/text/stable/datasets.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Audio dataset <a href="https://pytorch.org/audio/stable/datasets.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/audio/stable/datasets.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <blockquote><p>PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data.</p></blockquote> <blockquote><p>Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.</p></blockquote> <p>PyTorch的库提供了一些预加载的数据集，比如FashionMNIST。这些数据集可以用来快速实现模型以及benchmark模型。</p> <h3 id="加载数据集"><a href="#加载数据集" class="header-anchor">#</a> 加载数据集</h3> <p>这里以加载TorchVision中的<a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener noreferrer">Fashion-MNIST<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>为例.</p> <blockquote><p>Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.</p></blockquote> <p>We load the FashionMNIST Dataset with the following parameters:</p> <ul><li>root is the path where the train/test data is stored,</li> <li>train specifies training or test dataset,</li> <li>download=True downloads the data from the internet if it’s not available at root.</li> <li>transform and target_transform specify the feature and label transformations</li></ul> <div class="language- extra-class"><pre class="language-text"><code>import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt


training_data = datasets.FashionMNIST(
    root=&quot;data&quot;,
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root=&quot;data&quot;,
    train=False,
    download=True,
    transform=ToTensor()
)
</code></pre></div><p>这里的torchvision提供了很多内置的datasets在torchvision.datasets的模块中。所有的数据集都是<code>torch.utils.data.Dataset</code>的子集，也就是说他们具有<code>__getitem__</code>和<code>__len__</code>的方法。因此他们能够作为参数传给<code>torch.utils.data.DataLoader</code>.</p> <div class="language- extra-class"><pre class="language-text"><code>imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')
data_loader = torch.utils.data.DataLoader(imagenet_data,
                                          batch_size=4,
                                          shuffle=True,
                                          num_workers=args.nThreads)
</code></pre></div><h3 id="遍历数据集"><a href="#遍历数据集" class="header-anchor">#</a> 遍历数据集</h3> <p>可以通过类似数组一样去下表访问对应的值 <code>training_data[index]</code>.</p> <div class="language- extra-class"><pre class="language-text"><code>每一项是一个tuple，分别包括img的值和对应的label
img, label = training_data[sample_idx]
</code></pre></div><h3 id="自定义dataset"><a href="#自定义dataset" class="header-anchor">#</a> 自定义dataset</h3> <blockquote><p>A custom Dataset class must implement three functions: <strong>init</strong>, <strong>len</strong>, and <strong>getitem</strong>.</p></blockquote> <blockquote><p>Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file.</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code>import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
</code></pre></div><p><code>__init__</code></p> <blockquote><p>The <strong>init</strong> function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms.</p></blockquote> <p>The labels.csv file looks like:</p> <div class="language- extra-class"><pre class="language-text"><code>tshirt1.jpg, 0
tshirt2.jpg, 0
......
ankleboot999.jpg, 9
</code></pre></div><p><code>__len__</code></p> <p>The <strong>len</strong> function returns the number of samples in our dataset.</p> <p><code>__getitem__</code></p> <blockquote><p>The <strong>getitem</strong> function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple.</p></blockquote> <h4 id="使用dataloader去训练"><a href="#使用dataloader去训练" class="header-anchor">#</a> 使用DataLoader去训练</h4> <p>Dataset每次读取一个样本和其对应的label，但是常常在训练模型的时候，我们想以batch的方式去训练，并且在每一个epoch的时候reshuffle数据来降低过拟合的可能性。同时使用python的多线程来加速数据的读取。</p> <div class="language- extra-class"><pre class="language-text"><code>from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
</code></pre></div><h4 id="通过dataloader去遍历"><a href="#通过dataloader去遍历" class="header-anchor">#</a> 通过DataLoader去遍历</h4> <blockquote><p>We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code># Display image and label.
train_features, train_labels = next(iter(train_dataloader))
print(f&quot;Feature batch shape: {train_features.size()}&quot;)
print(f&quot;Labels batch shape: {train_labels.size()}&quot;)
img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap=&quot;gray&quot;)
plt.show()
print(f&quot;Label: {label}&quot;)

Feature batch shape: torch.Size([64, 1, 28, 28])
Labels batch shape: torch.Size([64])
Label: 5
</code></pre></div><h2 id="transforms"><a href="#transforms" class="header-anchor">#</a> Transforms</h2> <p>通过transforms来对数据进行操作，使其适合训练。所有的torchvision.datasets都具有两个参数，<code>transform</code>用于修改features, <code>target_transform</code>勇于修改labels.</p> <ul><li>torchvision.transforms <a href="https://pytorch.org/vision/stable/transforms.html" target="_blank" rel="noopener noreferrer">https://pytorch.org/vision/stable/transforms.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <p>比如FashionMNIST的features存储是以PIL图片格式，为了方便训练，我们需要normalized的张量格式，以及将labels存储为one-hot张量格式。为了处理最初的数据，我们使用<code>ToTensor</code>和<code>Lambda</code>去转变最初的数据.</p> <div class="language- extra-class"><pre class="language-text"><code>import torch
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda

ds = datasets.FashionMNIST(
    root=&quot;data&quot;,
    train=True,
    download=True,
    transform=ToTensor(),
    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))
)
</code></pre></div><h3 id="totensor"><a href="#totensor" class="header-anchor">#</a> ToTensor()</h3> <p>ToTensor将一个PIL图片或者NumPy数组转变成<code>FloatTensor</code>并且将图片的值<strong>转变到[0., 1.]之间</strong>。</p> <h3 id="lambda-transforms"><a href="#lambda-transforms" class="header-anchor">#</a> Lambda Transforms</h3> <blockquote><p>Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y.</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code>target_transform = Lambda(lambda y: 
torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))
</code></pre></div><p>torch.Tensor.scatter_ 是一个用于修改张量内容的 PyTorch 函数，通常用于高效地对张量的特定位置进行更新。这个函数是一个原地（in-place）操作，意味着它会直接修改调用它的张量，而不会创建新的张量。scatter_ 的用法可以有点复杂，但它非常有用，特别是在处理诸如构建索引或进行一些特殊形式的赋值时。</p> <p>功能和参数</p> <ul><li>scatter_ 的基本用途是根据索引从一个源张量中取值，并将这些值放置到当前张量的指定位置。其参数通常包括：</li> <li>dim：指定要在哪一个维度上进行操作。</li> <li>index：一个与原张量同形状的张量，包含了要更新的元素的索引。</li> <li>src：可以是一个与原张量同形状的张量或一个单一的标量值，表示要填充到指定位置的数据。</li></ul> <h3 id="transforms和transforms-v2"><a href="#transforms和transforms-v2" class="header-anchor">#</a> transforms和transforms.v2</h3> <blockquote><p>Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2 modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).</p></blockquote> <p>目前有两个版本，一个是torchvision.transforms另一个是torchvision.transforms.v2.</p> <p>为什么有两个版本</p> <ul><li>向后兼容性：原始的 torchvision.transforms 被广泛使用，在许多项目和代码库中都有实现。直接替换或大幅修改它可能会破坏现有代码。因此，新的 v2 模块提供了一个平滑过渡的路径，同时保持旧版本的功能。</li> <li>功能和设计改进：随着技术的发展和用户反馈的积累，torchvision 团队识别出了原有 transforms 的一些设计限制和性能瓶颈。v2 模块的引入允许他们在不影响现有用户的情况下，实现这些改进。</li></ul> <h4 id="transforms-v2"><a href="#transforms-v2" class="header-anchor">#</a> transforms v2</h4> <ul><li><a href="https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py" target="_blank" rel="noopener noreferrer">Getting started with transforms v2<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="构建神经网络模型"><a href="#构建神经网络模型" class="header-anchor">#</a> 构建神经网络模型</h2> <p>神经网络由模块(modules)/层(layers)所组成，<code>torch.nn</code>命名空间提供了所有的神经网络模块/层。每个模块都是<code>nn.Module</code>的子类。</p> <h3 id="获得硬件类型"><a href="#获得硬件类型" class="header-anchor">#</a> 获得硬件类型</h3> <div class="language- extra-class"><pre class="language-text"><code>device = (
    &quot;cuda&quot;
    if torch.cuda.is_available()
    else &quot;mps&quot;
    if torch.backends.mps.is_available()
    else &quot;cpu&quot;
)
print(f&quot;Using {device} device&quot;)
</code></pre></div><h3 id="定义class"><a href="#定义class" class="header-anchor">#</a> 定义Class</h3> <p>通过实现<code>nn.Module</code>子类来定义自己的神经网络模型，并且在<code>__init__</code>里面去初始化模型所需的模块。每个<code>nn.Module</code>子类需要在<code>forward</code>方法里面实现对输入数据的操作。</p> <div class="language- extra-class"><pre class="language-text"><code>class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
</code></pre></div><p>然后创建<code>NeuralNetwork</code>的实例，并将其移动到<code>device</code>上。</p> <div class="language- extra-class"><pre class="language-text"><code>model = NeuralNetwork().to(device)
print(model)

NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)
</code></pre></div><p>注意：对于tensor.to()函数是返回一个copy的值，而nn.Module.to()的函数是一个in-place修改。</p> <p>为了使用这个模型，我们将数据传递给模型。这个会执行模型的forward函数，注意不要直接传递数据给<code>model.forward()</code>.</p> <div class="language- extra-class"><pre class="language-text"><code>X = torch.rand(1, 28, 28, device=device)
logits = model(X)
pred_probab = nn.Softmax(dim=1)(logits)
y_pred = pred_probab.argmax(1)
print(f&quot;Predicted class: {y_pred}&quot;)

Predicted class: tensor([7], device='cuda:0')
</code></pre></div><p>通过将结果传递给<code>nn.Softmax</code>模块获得最终的预测概率。</p> <h3 id="模型解读"><a href="#模型解读" class="header-anchor">#</a> 模型解读</h3> <p>以刚刚定义的模型为例，解读数据传入之后进行的操作。这里以一个minibatch为3的样本为例，其中每张图片的大小为28x28。
<code>input_image = torch.rand(3,28,28)</code>.</p> <h4 id="nn-flatten"><a href="#nn-flatten" class="header-anchor">#</a> nn.Flatten</h4> <ul><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html" target="_blank" rel="noopener noreferrer">Flatten<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
我们初始化 nn.Flatten 层，将每个 2D 28x28 图像转换为 784 个像素值的连续数组（batch的维度dim=0会被保留）。</li></ul> <div class="language- extra-class"><pre class="language-text"><code>flatten = nn.Flatten()
flat_image = flatten(input_image)
print(flat_image.size())

torch.Size([3, 784])
</code></pre></div><p><code>torch.nn.Flatten(start_dim=1, end_dim=-1)</code></p> <ul><li>start_dim (int) – first dim to flatten (default = 1).</li> <li>end_dim (int) – last dim to flatten (default = -1).</li></ul> <div class="language- extra-class"><pre class="language-text"><code>&gt;&gt;&gt; input = torch.randn(32, 1, 5, 5)
&gt;&gt;&gt; # With default parameters
&gt;&gt;&gt; m = nn.Flatten()
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; output.size()
torch.Size([32, 25])
&gt;&gt;&gt; # With non-default parameters
&gt;&gt;&gt; m = nn.Flatten(0, 2)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; output.size()
torch.Size([160, 5])
</code></pre></div><h4 id="nn-linear"><a href="#nn-linear" class="header-anchor">#</a> nn.Linear</h4> <p>线性层是一个使用其存储的weights和bias对输入应用线性变换的模块。</p> <div class="language- extra-class"><pre class="language-text"><code>layer1 = nn.Linear(in_features=28*28, out_features=20)
hidden1 = layer1(flat_image)
print(hidden1.size())

torch.Size([3, 20])
</code></pre></div><p><code>torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)</code></p> <ul><li>in_features (int) – size of each input sample</li> <li>out_features (int) – size of each output sample</li> <li>bias (bool) – If set to False, the layer will not learn an additive bias. Default: True</li></ul> <p>关于线性层的weight和bias的初始化，可以看<a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" target="_blank" rel="noopener noreferrer">torch.nn.Linear<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</p> <h4 id="nn-relu"><a href="#nn-relu" class="header-anchor">#</a> nn.ReLU</h4> <blockquote><p>Non-linear activations are what create the complex mappings between the model’s inputs and outputs.
在线性变换后应用以引入非线性，帮助神经网络学习各种现象。</p></blockquote> <p>在线性层之间使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html" target="_blank" rel="noopener noreferrer">nn.ReLU<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>非线性激活函数，同时还有其他类型的激活函数。</p> <div class="language- extra-class"><pre class="language-text"><code>print(f&quot;Before ReLU: {hidden1}\n\n&quot;)
hidden1 = nn.ReLU()(hidden1)
print(f&quot;After ReLU: {hidden1}&quot;)
</code></pre></div><p><code>torch.nn.ReLU(inplace=False)</code></p> <ul><li>inplace (bool) – can optionally do the operation in-place. Default: False</li></ul> <h4 id="nn-sequential"><a href="#nn-sequential" class="header-anchor">#</a> nn.Sequential</h4> <p><code>nn.Sequential</code>是模块的有序容器。数据按照定义的相同顺序传递通过所有模块。</p> <div class="language- extra-class"><pre class="language-text"><code>seq_modules = nn.Sequential(
    flatten,
    layer1,
    nn.ReLU(),
    nn.Linear(20, 10)
)
input_image = torch.rand(3,28,28)
logits = seq_modules(input_image)
</code></pre></div><p><code>torch.nn.Sequential</code> A sequential container.</p> <div class="language- extra-class"><pre class="language-text"><code># Using Sequential to create a small model. When `model` is run,
# input will first be passed to `Conv2d(1,20,5)`. The output of
# `Conv2d(1,20,5)` will be used as the input to the first
# `ReLU`; the output of the first `ReLU` will become the input
# for `Conv2d(20,64,5)`. Finally, the output of
# `Conv2d(20,64,5)` will be used as input to the second `ReLU`
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Using Sequential with OrderedDict. This is functionally the
# same as the above code
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))
</code></pre></div><ul><li>append(module)</li></ul> <p>module (nn.Module) – module to append</p> <h4 id="nn-softmax"><a href="#nn-softmax" class="header-anchor">#</a> nn.Softmax</h4> <blockquote><p>The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1.</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code>softmax = nn.Softmax(dim=1)
pred_probab = softmax(logits)
</code></pre></div><h3 id="模型参数-model-parameters"><a href="#模型参数-model-parameters" class="header-anchor">#</a> 模型参数 Model Parameters</h3> <blockquote><p>Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.</p></blockquote> <p>In this example, we iterate over each parameter, and print its size and a preview of its values.</p> <div class="language- extra-class"><pre class="language-text"><code>print(f&quot;Model structure: {model}\n\n&quot;)

for name, param in model.named_parameters():
    print(f&quot;Layer: {name} | Size: {param.size()} | Values : {param[:2]} \n&quot;)

Model structure: NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)

Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],
        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115]],
       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0155, -0.0327], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0116,  0.0293, -0.0280,  ...,  0.0334, -0.0078,  0.0298],
        [ 0.0095,  0.0038,  0.0009,  ..., -0.0365, -0.0011, -0.0221]],
       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0256], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0147, -0.0229,  0.0180,  ..., -0.0013,  0.0177,  0.0070],
        [-0.0202, -0.0417, -0.0279,  ..., -0.0441,  0.0185, -0.0268]],
       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0070, -0.0411], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)
</code></pre></div><h2 id="autograd"><a href="#autograd" class="header-anchor">#</a> Autograd</h2> <ul><li><a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html" target="_blank" rel="noopener noreferrer">Automatic Differentiation with torch.autograd<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="optimization"><a href="#optimization" class="header-anchor">#</a> Optimization</h2> <p>训练模型是一个迭代过程；在每次迭代中，模型都会对输出进行猜测，计算其猜测的误差（损失），收集误差相对于其参数的导数，并使用梯度下降优化这些参数。</p> <div class="language- extra-class"><pre class="language-text"><code>import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

training_data = datasets.FashionMNIST(
    root=&quot;data&quot;,
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root=&quot;data&quot;,
    train=False,
    download=True,
    transform=ToTensor()
)

train_dataloader = DataLoader(training_data, batch_size=64)
test_dataloader = DataLoader(test_data, batch_size=64)

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork()
</code></pre></div><h3 id="超参数"><a href="#超参数" class="header-anchor">#</a> 超参数</h3> <ul><li><a href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html" target="_blank" rel="noopener noreferrer">Hyperparameter tuning with Ray Tune<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
超参数是可调整的参数，可以控制模型优化过程。不同的<a href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html" target="_blank" rel="noopener noreferrer">超参数<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>值会影响模型训练和收敛速度。</li></ul> <ul><li>Number of Epochs - the number times to iterate over the dataset</li> <li>Batch Size - the number of data samples propagated through the network before the parameters are updated</li> <li>Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.</li></ul> <div class="language- extra-class"><pre class="language-text"><code>learning_rate = 1e-3
batch_size = 64
epochs = 5
</code></pre></div><h3 id="optimization-loop"><a href="#optimization-loop" class="header-anchor">#</a> Optimization Loop</h3> <blockquote><p>Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an <strong>epoch</strong>.</p></blockquote> <p>Each epoch consists of two main parts:</p> <ul><li>The Train Loop - iterate over the training dataset and try to converge to optimal parameters.</li> <li>The Validation/Test Loop - iterate over the test dataset to check if model performance is improving.</li></ul> <h4 id="loss-function"><a href="#loss-function" class="header-anchor">#</a> loss function</h4> <ul><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss" target="_blank" rel="noopener noreferrer">MSELoss<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" target="_blank" rel="noopener noreferrer">NLLLoss<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <p>torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</p> <blockquote><p>The negative log likelihood loss. It is useful to train a classification problem with C classes. <br>
If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the  classes. This is particularly useful when you have an unbalanced training set.</p></blockquote> <ul><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener noreferrer">CrossEntropyLoss<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <p>torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)</p> <blockquote><p>The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general).</p></blockquote> <p>当提供一些训练数据时，我们未经训练的网络可能不会给出正确的答案。损失函数衡量的是得到的结果与目标值的不相似程度，它是我们在训练时想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。</p> <blockquote><p>Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.</p></blockquote> <p>We pass our model’s output logits to <code>nn.CrossEntropyLoss</code>, which will normalize the logits and compute the prediction error.</p> <div class="language- extra-class"><pre class="language-text"><code># Initialize the loss function
loss_fn = nn.CrossEntropyLoss()
</code></pre></div><h4 id="optimizer"><a href="#optimizer" class="header-anchor">#</a> Optimizer</h4> <ul><li><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener noreferrer">torch.optim<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <p>优化是调整模型参数以减少每个训练步骤中模型误差的过程。优化算法定义了如何执行此过程（在本例中我们使用随机梯度下降）。所有优化逻辑都封装在优化器对象中。这里，我们使用SGD优化器；此外，PyTorch 中还有许多不同的优化器，例如 ADAM 和 RMSProp，它们可以更好地处理不同类型的模型和数据。</p> <blockquote><p>We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.</p></blockquote> <div class="language- extra-class"><pre class="language-text"><code>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
</code></pre></div><p>Inside the training loop, optimization happens in three steps:</p> <ol><li>Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.</li> <li>Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.</li> <li>Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.</li></ol> <h3 id="full-implementation"><a href="#full-implementation" class="header-anchor">#</a> Full implementation</h3> <p>We define train_loop that loops over our optimization code, and test_loop that evaluates the model’s performance against our test data.</p> <div class="language- extra-class"><pre class="language-text"><code>def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    # Set the model to training mode - important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * batch_size + len(X)
            print(f&quot;loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]&quot;)


def test_loop(dataloader, model, loss_fn):
    # Set the model to evaluation mode - important for batch normalization and dropout layers
    # Unnecessary in this situation but added for best practices
    model.eval()
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode
    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True
    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f&quot;Test Error: \n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \n&quot;)
</code></pre></div><p>We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the model’s improving performance.</p> <div class="language- extra-class"><pre class="language-text"><code>loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epochs = 10
for t in range(epochs):
    print(f&quot;Epoch {t+1}\n-------------------------------&quot;)
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print(&quot;Done!&quot;)
</code></pre></div><h2 id="保存和加载模型"><a href="#保存和加载模型" class="header-anchor">#</a> 保存和加载模型</h2> <p>在本节中，我们将了解如何持久保存模型状态、加载模型和运行模型预测。</p> <h3 id="保存和加载模型权重"><a href="#保存和加载模型权重" class="header-anchor">#</a> 保存和加载模型权重</h3> <p>PyTorch模型存储学习到的参数在一个内置的状态字典里 <code>state_dict</code>. 他们可以通过<code>torch.save</code>方法进行保存。</p> <div class="language- extra-class"><pre class="language-text"><code>model = models.vgg16(weights='IMAGENET1K_V1')
torch.save(model.state_dict(), 'model_weights.pth')
</code></pre></div><p>为了加载模型的权重，我们首先需要创立一个与对应的模型一样的模型实例，然后通过<code>load_state_dict()</code>方法去加载保存的模型参数。</p> <div class="language- extra-class"><pre class="language-text"><code>model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()
</code></pre></div><p>注意：加载模型参数之后，如果需要进行推断，则需要将模型设置为eval()模式，这样会把dropout和BN层设置为evaluation模式。否则模型可能会产生不一致的结果。</p> <h3 id="直接保存模型以及其权重"><a href="#直接保存模型以及其权重" class="header-anchor">#</a> 直接保存模型以及其权重</h3> <p>我们也许想直接加载一整个模型以及其权重，而不是首先实例模型，然后加载参数。</p> <p>通过将模型本身传入到<code>save</code>方法中，而不是<code>model.state_dict()</code>可以实现这个功能：</p> <div class="language- extra-class"><pre class="language-text"><code>torch.save(model, 'model.pth')
</code></pre></div><p>加载模型时需要注意是通过另一个函数：</p> <div class="language- extra-class"><pre class="language-text"><code>model = torch.load('model.pth')
</code></pre></div><h2 id="代码复现"><a href="#代码复现" class="header-anchor">#</a> 代码复现</h2></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/learning-blogs/pytorch/" class="prev router-link-active">
        安装
      </a></span> <span class="next"><a href="/learning-blogs/pytorch/hyperparameter_tuning.html">
        超参数优化
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/learning-blogs/assets/js/app.023149a4.js" defer></script><script src="/learning-blogs/assets/js/2.4cc10ed0.js" defer></script><script src="/learning-blogs/assets/js/1.91ea3048.js" defer></script><script src="/learning-blogs/assets/js/48.fdc47c13.js" defer></script>
  </body>
</html>
